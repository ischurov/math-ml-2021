{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение\n",
    "\n",
    "## Факультет математики НИУ ВШЭ\n",
    "\n",
    "### 2020-2021 учебный год\n",
    "\n",
    "Лектор: Илья Щуров\n",
    "\n",
    "Семинаристы: Руслан Хайдуров, Соня Дымченко\n",
    "\n",
    "Ассистенты: Максим Бекетов, Павел Егоров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы:\n",
    "\n",
    "- познакомимся с автоматической обработкой текстов\n",
    "- рассмотрим базовые методы для работы с текстами\n",
    "- решим задачу анализа тональности текстов благодаря полученным навыкам!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры задач автоматической обработки текстов:\n",
    "\n",
    "- классификация текстов\n",
    "\n",
    "    - анализ тональности\n",
    "    - фильтрация спама\n",
    "    - по теме или жанру\n",
    "\n",
    "- машинный перевод\n",
    "\n",
    "- распознавание речи\n",
    "\n",
    "- извлечение информации\n",
    "\n",
    "    - именованные сущности\n",
    "    - факты и события\n",
    "\n",
    "- кластеризация текстов\n",
    "\n",
    "- оптическое распознавание символов\n",
    "\n",
    "- проверка правописания\n",
    "\n",
    "- вопросно-ответные системы\n",
    "\n",
    "- суммаризация текстов\n",
    "\n",
    "- генерация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня рассмотрим следующие приемы для работы с текстами:\n",
    "\n",
    "**Предобработка текста**\n",
    "- токенизация\n",
    "- лемматизация / стемминг\n",
    "- удаление стоп-слов\n",
    "\n",
    "**Векторные представления текстов**\n",
    "\n",
    "- bag of words\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текста\n",
    "\n",
    "## 1 Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы качественно проанализировать текст, нужно разграничить в нем токены - отдельные слова. Процесс такого разбиения называется ***токенизацией***. В данный метод также может включаться избавление от знаков препинания и преобразование заглавных букв в строчные.\n",
    "\n",
    "Рассмотрим небольшой пример. Предположим, нужно проделать токенизацию первых двух предложений из романа Фрэнсиса Скотта Фицджеральда \"Великий Гэтсби\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\n",
      "\"Whenever you feel like criticizing any one,\" he told me, \"just remember that all the people in this world haven't had the advantages that you've had.\"\n"
     ]
    }
   ],
   "source": [
    "text = '''In my younger and more vulnerable years my father gave me some advice that \\\n",
    "I've been turning over in my mind ever since.\\n\\\"Whenever you feel like criticizing any one,\\\n",
    "\\\" he told me, \\\"just remember that all the people in this world haven't had the advantages that you've had.\\\"'''\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разделить текст по пробелам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', \"I've\", 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since.', '\"Whenever', 'you', 'feel', 'like', 'criticizing', 'any', 'one,\"', 'he', 'told', 'me,', '\"just', 'remember', 'that', 'all', 'the', 'people', 'in', 'this', 'world', \"haven't\", 'had', 'the', 'advantages', 'that', \"you've\", 'had.\"']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# re.split(\"[,\\t\\n\\ \\\".]+\", text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что вместе со словами выделились и знаки пунктуации. Можно просто убрать их вручную (или воспользовать регулярными выражениями), однако реальные *данные наполнены различным шумом* (html-разметка, ссылки, лишние знаки пунктуации) и опечатками, что создает дополнительные трудности.\n",
    "\n",
    "Во избежание вышеупомянутых проблем, для токенизации используются морфологические правила, основанные на регулярных выражениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/svdcvt/miniconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: click in /home/svdcvt/miniconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/svdcvt/miniconda3/lib/python3.8/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /home/svdcvt/.local/lib/python3.8/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/svdcvt/.local/lib/python3.8/site-packages (from nltk) (4.54.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/svdcvt/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'i', \"'ve\", 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', '.', '``', 'whenever', 'you', 'feel', 'like', 'criticizing', 'any', 'one', ',', \"''\", 'he', 'told', 'me', ',', '``', 'just', 'remember', 'that', 'all', 'the', 'people', 'in', 'this', 'world', 'have', \"n't\", 'had', 'the', 'advantages', 'that', 'you', \"'ve\", 'had', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно оставить токены, содержащие только буквы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'i', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', 'whenever', 'you', 'feel', 'like', 'criticizing', 'any', 'one', 'he', 'told', 'me', 'just', 'remember', 'that', 'all', 'the', 'people', 'in', 'this', 'world', 'have', 'had', 'the', 'advantages', 'that', 'you', 'had']\n"
     ]
    }
   ],
   "source": [
    "text_tokenized = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Лемматизация и стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После токенизации можно применить лемматизацию и/или стемминг.\n",
    "\n",
    "**Лемматизация** - процедура, при которой все выделенные словоформы _приводятся к своим леммам_ (нормальным формам).\n",
    "\n",
    "> Например, токены \"пью\", \"пил\", \"пьет\" перейдут в \"пить\".  \n",
    "\n",
    "Здесь, как и в токенизации, возникают неопределенности, связанные с зависимостью смыслов слов от контекста: например, \"рой\" может быть глаголом в повелительном наклонении, образованным от глагола \"рыть\", или же существительным в именительном падеже (\"пчелиный рой\"). Неопределенности можно разрешить с помощью вероятностной модели, которая будет рассматривать контекст (слова, расположенные рядом с данным) и определять, с какой вероятностью данное слово имеет тот или иной смысл.\n",
    "\n",
    "При применении **стемминга** у всех слов _отбрасываются аффиксы_ (окончания и суффиксы). \n",
    "\n",
    "> Например, токены \"идет\", \"идущий\", \"идя\", \"идут\" перейдет в \"ид\".\n",
    "\n",
    "То, что осталось от слова по окончании процедуры, называют **стемом**. Приводя различные синтетические формы одного и того же слова к одному виду, стемминг существенно может улучшить качество модели. Однако здесь тоже встречаются неопределенности: например, \"белка\", \"белый\" и \"белье\" при тривиальном стемминге переходят в \"бел\", \"скорый\" и \"поскорее\" переходят в разные стемы \"скор\" и \"поскор\". Эти неопределенности можно разрешить путем последовательного применения ряда морфологических правил.\n",
    "\n",
    "#### Зачем это делать?\n",
    "\n",
    "1. Уменьшение размера словаря\n",
    "2. Это может быть необходимо в некоторых задачах, например\n",
    "    - Создание классов эквивалентности в __информационном поиске__:\n",
    "        - кошка, кошки, кошку, кошкой, кошке… -> кошка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/svdcvt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulnerable years my father gave me some advice that i been turning over in my mind ever since whenever you feel like criticizing any one he told me just remember that all the people in this world have had the advantages that you had\n"
     ]
    }
   ],
   "source": [
    "print(*text_tokenized, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulner year my father gave me some advic that i been turn over in my mind ever sinc whenev you feel like critic ani one he told me just rememb that all the peopl in this world have had the advantag that you had\n"
     ]
    }
   ],
   "source": [
    "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
    "print(*text_stemmed, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulnerable year my father gave me some advice that i been turning over in my mind ever since whenever you feel like criticizing any one he told me just remember that all the people in this world have had the advantage that you had\n"
     ]
    }
   ],
   "source": [
    "text_lemmatized = [lemmatizer.lemmatize(w) for w in text_tokenized]\n",
    "print(*text_lemmatized, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "years - year - year\n",
    "criticizing - critic - criticizing\n",
    "any - ani - any\n",
    "remember - rememb - remember\n",
    "advantages - advantag - advantage\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности работы с русскими текстами\n",
    "\n",
    "1. Используется в основном лемматизация, а не стемминг, потому что хорошего стеммера для руссого нет (Snowball тоже работает с русским, но обработка лемматизацией работает лучше)\n",
    "\n",
    "2. Для лемматизации используется либо [PyMorphy](https://nlpub.ru/Pymorphy), либо [MyStem](https://nlpub.ru/Mystem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тексте могут встречаться слова, не несущие в себе абсолютно никакой информации - шумовые, или стоп-слова. Их можно отфильтровать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/svdcvt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger vulner year father gave advic turn mind ever sinc whenev feel like critic ani one told rememb peopl world advantag\n"
     ]
    }
   ],
   "source": [
    "text_stemmed_stopped = [w for w in text_stemmed if w not in stop_words]\n",
    "print(*text_stemmed_stopped, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger vulnerable year father gave advice turning mind ever since whenever feel like criticizing one told remember people world advantage\n"
     ]
    }
   ],
   "source": [
    "text_lemmatized_stopped = [w for w in text_lemmatized if w not in stop_words]\n",
    "print(*text_lemmatized_stopped, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger vulnerable year father gave advice turning mind ever since whenever feel like criticizing one told remember people world advantage\n"
     ]
    }
   ],
   "source": [
    "print(*[w.lower() for w in text_lemmatized if w.lower() not in stop_words], sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторные представления текстов\n",
    "\n",
    "Но как же все-таки работать с текстами, используя стандартные методы машинного обучения? Нужна выборка!\n",
    "\n",
    "## Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст можно представить как набор независимых слов. Тогда каждому слову можно сопоставить вес, таким образом, сопоставляя тексту набор весов. В качестве весов можно брать частоту встречаемости слов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I like my cat.', 'My cat is the most perfect cat.', 'is this cat or is this bread?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like my cat',\n",
       " 'My cat is the most perfect cat',\n",
       " 'is this cat or is this bread']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like my cat',\n",
       " 'my cat is the most perfect cat',\n",
       " 'is this cat or is this bread']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized = [' '.join([stemmer.stem(w) for w in word_tokenize(t) if w.isalpha()]) for t in texts]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like cat', 'my cat perfect cat', 'cat bread']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized = [' '.join([stemmer.stem(w) for w in word_tokenize(t) if w.isalpha() and w not in stop_words]) for t in texts]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обычно, выручает `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# метод подсчитывает количество слов всего и подсчитывает веса для каждого примера (предложения/текста)\n",
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(texts_tokenized) # сжатая матрица весов (sparse matrix, потому что очень много нулей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.csr.csr_matrix, (3, 10))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this'],\n",
       " 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec.get_feature_names(), len(cnt_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0],\n",
       "       [0, 2, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что если слово часто встречается в одном тексте, но почти не встречается в других, то оно получает для данного текста большой вес, ровно так же, как и слова, которые часто встречаются в каждом тексте. \n",
    "Для того, чтобы разделять такие слова - популярные среди текстов в принципе и популярные-уникальные для текста, можно использовать **статистическую меру TF-IDF, характеризующую важность слова для конкретного текста**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D_1 = 'like my cat'\n",
    "\n",
    "D_2 = 'my cat is the most perfect cat'\n",
    "\n",
    "D_3 = 'is this cat or is this bread'\n",
    "\n",
    "word |frequency in D1| D2 | D3\n",
    "---  | ---           | ---| ---\n",
    "bread| 0             | 0 | 1\n",
    "cat  | 1             | 2 | 1\n",
    "is   | 0 | 1 | 2\n",
    "like | 1 | 0 |0 \n",
    "most | 0 | 1 | 0\n",
    "my   | 1 | 1 | 0\n",
    "or   | 0 | 0 | 1\n",
    "perfect | 0 | 1 | 0\n",
    "the  | 0 | 1 | 0\n",
    "this | 0 | 0 | 2\n",
    "---- | --- | --- | --- |\n",
    "sum()| 3 | 7 | 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого слова $t$ из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{C(t)}{\\sum\\limits_{k \\in d}C(k)} = \\frac{\\text{число вхождений слова $t$ в текст $d$}}{\\text{количество всех слов в тексте $d$}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d_i \\in D \\mid t \\in d_i\\}|}\\right) = \\log(\\frac{\\text{общее число документов}}{\\text{число документов, куда входит слово $t$}})\n",
    "$$\n",
    "\n",
    "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге каждому слову $t$ из текста $d$ теперь можно присвоить вес\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "\n",
    "Интерпретировать формулу выше несложно: действительно, чем чаще данное слово встречается в данном тексте и чем реже в остальных, тем важнее оно для этого текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**А что там с практикой? `sklearn`, на помощь!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# метод возвращает матрицу с весами для слов для каждого примера (текста/предложения)\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.csr.csr_matrix, (3, 10))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42544054, 0.        , 0.72033345, 0.        ,\n",
       "        0.54783215, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.50130994, 0.32276391, 0.        , 0.42439575,\n",
       "        0.32276391, 0.        , 0.42439575, 0.42439575, 0.        ],\n",
       "       [0.33976626, 0.20067143, 0.516802  , 0.        , 0.        ,\n",
       "        0.        , 0.33976626, 0.        , 0.        , 0.67953252]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что изменилось по сравнению с методом `CountVectorizer`? Интерпретируйте результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам предлагается решить задачу анализа тональности - построить модель, определяющую по отзыву о фильме, положительный он или отрицательный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('movie_reviews.csv')\n",
    "except:\n",
    "    data = pd.read_csv('https://raw.githubusercontent.com/ischurov/math-ml-hse-2018/master/sem08_texts/movie_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tristar / 1 : 30 / 1997 / r ( language , viole...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arlington road 1/4 . directed by mark pellingt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the brady bunch movie is less a motion picture...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>janeane garofalo in a romantic comedy -- it wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'm going to keep this plot summary brief , so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  positive\n",
       "0  tristar / 1 : 30 / 1997 / r ( language , viole...         0\n",
       "1  arlington road 1/4 . directed by mark pellingt...         0\n",
       "2  the brady bunch movie is less a motion picture...         0\n",
       "3  janeane garofalo in a romantic comedy -- it wa...         0\n",
       "4  i'm going to keep this plot summary brief , so...         0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tristar, /, 1, :, 30, /, 1997, /, r, (, langu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[arlington, road, 1/4, ., direct, mark, pellin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bradi, bunch, movi, le, motion, pictur, minor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[janean, garofalo, romant, comedi, --, good, i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['m, go, keep, plot, summari, brief, ,, someth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>[one, last, entri, long-run, carri, seri, ,, c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>[hype, ?, sheesh, ,, like, ., side, titan, ,, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>[u, n't, yet, born, 1960, 's, rock, ', n, ', r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>[start, monoton, talking-head, music, histori,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>[jacki, brown, (, miramax, -, 1997, ), star, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  positive\n",
       "0     [tristar, /, 1, :, 30, /, 1997, /, r, (, langu...         0\n",
       "1     [arlington, road, 1/4, ., direct, mark, pellin...         0\n",
       "2     [bradi, bunch, movi, le, motion, pictur, minor...         0\n",
       "3     [janean, garofalo, romant, comedi, --, good, i...         0\n",
       "4     ['m, go, keep, plot, summari, brief, ,, someth...         0\n",
       "...                                                 ...       ...\n",
       "1395  [one, last, entri, long-run, carri, seri, ,, c...         1\n",
       "1396  [hype, ?, sheesh, ,, like, ., side, titan, ,, ...         1\n",
       "1397  [u, n't, yet, born, 1960, 's, rock, ', n, ', r...         1\n",
       "1398  [start, monoton, talking-head, music, histori,...         1\n",
       "1399  [jacki, brown, (, miramax, -, 1997, ), star, p...         1\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) токенизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) лемматизировать или стемматизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) TfidfVectorizer() или CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) в итоге получить матрицу X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) разделить выбору на трейн-тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) обучить классификатор из sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) посчитать метрики для каждого классификатора и сравнить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (*) посмотреть какой вес дает logreg словам (самый большой и самый маленький)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
