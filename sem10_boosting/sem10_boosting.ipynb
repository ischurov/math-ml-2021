{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение\n",
    "\n",
    "## Факультет математики НИУ ВШЭ\n",
    "\n",
    "### 2020-2021 учебный год\n",
    "\n",
    "Илья Щуров, Соня Дымченко, Руслан Хайдуров, Павел Егоров, Максим Бекетов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 10. Композиции алгоритмов, продолжение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы познакомимся с алгоритмом ансамблирования решающих деревьев gradient boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск\n",
    "\n",
    "Градиентный спуск мы изучили на предыдщуих семинарах. Самый простой метод минимизации функции ошибки (квадратичной, например) в пространстве параметров (веса $w_i$ в линейной модели, например), для оптимизации в каждый момент времени двигаемся по антиградиенту функции с каким-то шагом $\\eta$. \n",
    "\n",
    "$$w_{n+1} = w_n - \\eta \\cdot \\frac{\\partial f}{\\partial w}$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный бустинг\n",
    "\n",
    "Теперь давайте представим, что на каждом шаге мы оптимизируем функционал ошибки $L(y, a(x))$ не в пространстве параметров алгоритма, а в пространстве функций, чтобы найти приближение $\\hat f(x)$. Тогда на $N$-ом шаге, мы получим композицию алгоритмов (функций), которые также называют weak learners.\n",
    "\n",
    "**Алгоритм**:\n",
    "\n",
    "1. Первый алгоритм $b_0(X)$ предсказывает константу:\n",
    "    $$a_0(x) = b_0(x) = \\arg\\min_c \\sum_{k=1}^K L(y_k, c)$$\n",
    "2. На каждом шаге $i = 1, \\dots, N$:\n",
    "\n",
    "    1) Вычисляем остатки\n",
    "$$z^{i} = -\\frac{\\partial L(y, a_{i-1}(x))}{\\partial a_{i-1}(x)}$$\n",
    "    2) Обучаем модель $b_i(x)$ предсказывать $z^{i}$:\n",
    "$$b_i(x) = \\arg\\min_{\\theta_b} \\sum_{k=1}^K(z_k^{i} - b_i(x_k))^2$$\n",
    "    3) Подбираем коэффициент $\\gamma_i$ одномерной минимизацией: \n",
    "$$\\gamma_i = \\arg\\min_{\\gamma}\\sum_{k=0}^K L(y_k, a_{i-1}(x_k) + \\gamma b_i(x_k)$$\n",
    "    4) Дополняем композицию:\n",
    "$$a_i = a_{i-1} + \\gamma_i b_i(x)$$\n",
    "3. Получаем итоговый алгоритм:\n",
    "$$a_N(x) = \\sum_{i=0}^N\\gamma_i b_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Визуализации](http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем градиентный бустинг для бинарной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "\n",
    "Выпишите формулу градиента функции ошибки - бинарной кросс-энтропии - по ответам модели $a(x) = \\hat{y}$;\n",
    "\n",
    "$$L(y, \\hat(y)) = - y \\cdot \\log (\\hat{y}) - (1-y) \\cdot \\log (1-\\hat{y}) $$\n",
    "\n",
    "$$\\frac{\\partial L(y, \\hat y)}{\\partial \\hat y} = ?$$\n",
    "\n",
    "Реализуйте полученную формулу в виде функции, возвращающей список из **анти**градиентов для каждого предсказания модели (длина выборки $K$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_grad(y_true, y_pred):\n",
    "    # YOUR CODE\n",
    "    dloss = 0.\n",
    "    # YOUR CODE\n",
    "    return dloss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = loss_grad(np.array([0, 0, 0, 0, 1, 1, 1]),\n",
    "                np.array([0.2, 0.2, 0.5, 0.5, 0.2, 0.8, 0.8]))\n",
    "assert np.all(np.isclose(res, np.array([-1.25, -1.25, -2., -2., -5., -1.25, -1.25])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = loss_grad(np.array([0, 0, 0, 0, 1, 1, 1]),\n",
    "                np.array([0., 0.2, 0.5, 0.5, 0.2, 1., 0.8]))\n",
    "assert not any(np.isnan(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "\n",
    "Реализуйте функцию, которая делает предсказание, принимая на вход матрицу признаков `X` (для которой делается предсказание), а также список обученных элементарных алгоритмов `estimators` и их веса `gammas`. \n",
    "\n",
    "Это нужно для шага 2.1, где мы вычисляем ответы уже построенного ансамбля, чтобы по ним посчитать антиградиент функции ошибки - остатки $z^i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, estimators, gammas):\n",
    "    '''\n",
    "    X: np.array of size KxM\n",
    "    estimators: list with sklearn models of size N with .predict method\n",
    "    gammas: list of size N\n",
    "    \n",
    "    return: np.array of size K\n",
    "    '''\n",
    "    # YOUR CODE\n",
    "    y_pred = \n",
    "    # YOUR CODE\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.\n",
    "\n",
    "Реализуйте функцию обучения, которая принимает на вход признаки `X` и метки `original_y`, а возвращает список из обученных базовых алгоритмов `estimators` и весов этих алгоритмов `gammas`. \n",
    "\n",
    "Используйте данную функцию `get_weight` для получения оптимального веса нового базового алгоритма в ансамбле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательная функция для нахождения веса \\gamma нового базового алгоритма в ансамбле\n",
    "# Это есть шаг 2.3\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\n",
    "def get_weight(y, y_pred, y_prev_pred):\n",
    "    \"\"\"\n",
    "    Решает задачу одномерной оптимизации (минимизации mse) с помощью scipy.optimize.minimize_scalar\n",
    "    для нахождения оптимального веса gamma предсказаний нового алгоритма\n",
    "    \n",
    "    :param y: правильный ответ на объекте выборки\n",
    "    :param y_pred: предсказание нового базового алгоритма на объекте выборки\n",
    "    :param y_prev_pred: предсказание предыдущего ансамбля на этом объекте\n",
    "    :return: optimal gamma\n",
    "    \"\"\"\n",
    "    def mse(gamma, y, y_pred, y_prev_pred):\n",
    "        \"\"\"\n",
    "        Рассчитывает ошибку для данного веса gamma\n",
    "        нового предсказания y_pred\n",
    "\n",
    "        \"\"\"\n",
    "        # YOUR CODE\n",
    "        return \n",
    "    \n",
    "    # YOUR CODE\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensemble(X, y_true, n_estimators, base_estimator, lr):\n",
    "    # Храните базовые алгоритмы тут\n",
    "    estimators = []\n",
    "    # А их веса здесь\n",
    "    gammas = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        # Посчитайте градиент по предсказаниям текущего ансамбля\n",
    "        # Шаг 2.1\n",
    "        # YOUR CODE\n",
    "        grad = # YOUR CODE\n",
    "        \n",
    "        # обучите базовый алгоритм\n",
    "        # Шаг 2.2\n",
    "        # YOUR CODE\n",
    "        \n",
    "        # получите его вес gamma\n",
    "        # Шаг 2.3\n",
    "        # YOUR CODE\n",
    "        \n",
    "        # сохраните результаты итерации        \n",
    "        \n",
    "    return estimators, gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь соберем из этого одну сущность, которая будет обучаться\n",
    "\n",
    "Сущность, конечно, не идеальна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDT:\n",
    "    def __init__(self, n_estimators, base_estimator, lr):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.lr = lr\n",
    "        self.estimators = []\n",
    "        self.gammas = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # YOUR CODE\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X, y = make_classification(n_samples=600, n_features=2,\n",
    "                           n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2,\n",
    "                           flip_y=0.05, class_sep=0.9, random_state=241)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "pred = dt.predict(X_test)\n",
    "print('ROCAUC of simple Decision Tree:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'n_estimators': 100,\n",
    "    'base_estimator': DecisionTreeRegressor(),\n",
    "    'lr': 0.05\n",
    "}\n",
    "\n",
    "gbdt = GBDT(**hyperparameters)\n",
    "gbdt.fit(X_train, y_train)\n",
    "pred = gbdt.predict(X_test)\n",
    "print('ROCAUC of our Gradient Boosting:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=hyperparameters['n_estimators'])\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "print('ROCAUC of Sklearn Random Forest:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=hyperparameters['n_estimators'])\n",
    "gb.fit(X_train, y_train)\n",
    "pred = gb.predict(X_test)\n",
    "print('ROCAUC of Sklearn Gradient Boosting:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы градиентного бустинга, но с модификациями, которые делают их в разы мощнее:\n",
    "- [XGBoost](https://arxiv.org/abs/1603.02754) (оптимизации второго порядка)\n",
    "- [LightGBM](http://www.audentia-gestion.fr/MICROSOFT/lightgbm.pdf) (особенное построение решающего дерева)\n",
    "- [CatBoost](https://arxiv.org/abs/1810.11363) (особенные кодировки категориальных данных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Визуализируем предсказания на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_surface(X, y, clf, ax):\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "for clf, ax in zip([dt, gbdt, rf, gb], axes.ravel()):\n",
    "    plot_surface(X_train, y_train, clf, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью градиентного бустинга также можно находить выбросы в данных; выбросами будут те точки, градиент на которых максимален по модулю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
