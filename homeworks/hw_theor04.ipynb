{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Машинное обучение \n",
    "### Факультет математики НИУ ВШЭ, 2020-21 учебный год\n",
    "\n",
    "_Илья Щуров, Соня Дымченко, Руслан Хайдуров, Максим Бекетов, Павел Егоров_\n",
    "\n",
    "[Страница курса](http://wiki.cs.hse.ru/Машинное_обучение_на_матфаке_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 4. Вокруг линейных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание выполнил(а): _(впишите свои фамилию и имя)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Внимание!__ Домашнее задание выполняется самостоятельно. При попытке сдать хотя бы частично списанный текст, или текст, полученный в результате совместного решения задач, вся работа будет оценена на 0 баллов. Мы также уведомим администрацию факультета и попросим применить дисциплинарное взыскание (предупреждение, выговор, отчисление) ко всем вовлеченным студентам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1 (20 баллов)\n",
    "Рассмотрим следующую модель. Значения $x_1, \\ldots, x_n \\in \\mathbb R^d$ фиксированы. Вектор $w \\in \\mathbb R^d$ фиксирован. Также фиксирован вектор $\\sigma = (\\sigma_1, \\ldots, \\sigma_n) \\in \\mathbb R^n$. Значения $y_i$ определяются следующим образом:\n",
    "\n",
    "$$\\newcommand{\\eps}{\\varepsilon}y_i = \\langle w, x_i \\rangle + \\eps_i,$$\n",
    "\n",
    "где $\\eps_i$ — независимые случайные величины, распределённые по нормальному закону, $\\eps_i \\sim \\mathcal N(0, \\sigma_i^2)$ (то есть у каждого $\\eps_i$ своя дисперсия, равная $\\sigma_i^2$, все $\\sigma_i$ фиксированы и известны).\n",
    "\n",
    "1. Найти функцию правдоподобия $p((y_1, \\ldots, y_n) \\mid w, x, \\sigma)$, равную плотности вероятности получения данных $y_1, \\ldots, y_n$ при заданных фиксированных $w$, $x$ и $\\sigma$.\n",
    "2. Найти логарифм правдоподобия.\n",
    "3. Записать задачу максимизации логарфима правдоподобия по $w$. Выкинуть лишние слагаемые и записать аналог $RSS$ для этой задачи. \n",
    "4. Записать задачу максимизации правдоподобия в матричном виде. Для этого ввести матрицы $X$ (матрица объект-признак, по строкам записаны $x_1, \\ldots, x_n$) и $\\Sigma$ — диагональная матрица, у которой на диагонали стоят $\\sigma_1, \\ldots, \\sigma_n$.\n",
    "5. Решить эту задачу в матричном виде. (Найти градиент аналога $RSS$ в матричном виде, приравнять нулю, решить получившееся уравнение. Найти гессиан, показать, что он отрицательно определён в точке максимума.)\n",
    "6. Является ли полученная оценка для $w$ несмещённой?\n",
    "7. Найти ковариационную матрицу для оценки $w$.\n",
    "\n",
    "**Подсказка.** Для самопроверки можеет подставить в качестве вектора $\\sigma$ постоянный вектор (все компоненты равны одному и тому же числу). Должны получиться формулы, которые доказывались на лекциях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(впишите решение сюда)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2 (10 баллов)\n",
    "Рассмотрим такую модель. Значения $x_1, \\ldots, x_n \\in \\mathbb R$ — фиксированные числа, $\\beta \\in \\mathbb R$ — фиксированное число, $\\eps_i \\sim \\mathcal N(0, 1)$ — независимые случайные ошибки,\n",
    "\n",
    "$$y_i = \\beta x_i + \\eps_i, \\quad i = 1, \\ldots, n.$$\n",
    "\n",
    "Пусть $\\widehat \\beta$ — МНК-оценка $\\beta$ для данной модели. Для предсказания значения $y$ в точке $x_{new}$ используется следующий алгоритм:\n",
    "\n",
    "$$\\widehat y_{new} = \\gamma \\cdot \\widehat \\beta x_{new},$$\n",
    "\n",
    "где $\\gamma \\in \\mathbb R$ — некоторая константа (не зависящая от $x_1, \\ldots, x_n$ и $y_1, \\ldots, y_n$).\n",
    "\n",
    "1. Для заданного $x_{new}$ найти квадрат смещения предсказания и разброс (дисперсию) предсказания (как функции от $x_{new}$, $\\beta$ и $\\gamma$ и $(x_1, \\ldots, x_n)$).\n",
    "2. Найти такое значение $\\gamma$, при котором ожидаемая квадратичная ошибка предсказания минимальна. (Эта величина будет зависеть от $\\beta$, $x_{new}$ и $(x_1, \\ldots, x_n)$)\n",
    "\n",
    "(Эта задача демонстрирует ещё одно проявление bias-variance tradeoff: мы можем пожертвовать несмещённостью, чтобы уменьшить ошибку предсказания.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(впишите решение сюда)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3 (10 баллов)\n",
    "Маша, Неля и Катя решают задачу линейной регрессии. Данные у них одинаковые, в них $n$ наблюдений и два признака $x^{(1)}$ и $x^{(2)}$, а также вектор ответов $y$. Признаки имеют нулевое выборочное среднее и нулевую [выборочную ковариацию](https://ru.wikipedia.org/wiki/Ковариация#Ковариация_выборок). Маша находит вектор весов $(w^{М}_1, w^{М}_2)$ как МНК-оценку для задачи $y_i=w_1 x^{(1)}_i+w_2 x^{(2)}_i+\\eps_i$. Неля решила выбросить второй признак и находит вес $w^{Н}_1$ как МНК-оценку для задачи $y_i=w_1 x^{(1)}_i + \\eps_i$. Катя выбросила первый признак и находит вес $w^{К}_2$ как МНК-оценку для задачи $y_i = w_2 x^{(2)}_i + \\eps_i$. Докажите, что $w^{М}_1 = w^{Н}_1$ и $w^{К}_2 = w^{М}_2$. Будет ли это верно в случае, если признаки будут по-прежнему иметь нулевое среднее, но окажутся скоррелированными (то есть не будут иметь нулевую ковариацию)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(впишите решение сюда)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4 (10 баллов)\n",
    "Маша и Катя решают задачу линейной регрессии. Изначально у них одинаковый набор данных, состоящий из $n$ наблюдений $x_i$, $i=1, \\ldots, x_n$ по $d$ признаков и вектора ответов $y=(y_1, \\ldots, y_n)$. Маша записала линейную модель\n",
    "\n",
    "$$y_i = x^{(1)}_i w_1 + \\ldots + x^{(d)}_i w_d + \\eps_i.$$\n",
    "\n",
    "и стала искать МНК-оценку для $(w_1, \\ldots, w_d)$. А Катя считает, что реальная зависимость между $y$ и признаками является нелинейной, поэтому она добавила новые признаки в модель (но не стала убирать старые). В качестве новых признаков она использовала различные линейные и нелинейные функции от старых признаков, которые ей приходили в голову. Таким образом, Катина модель выглядит так: \n",
    "\n",
    "$$y_i = x^{(1)}_i w_1 + \\ldots + x^{(d)}_i w_d + x^{(d+1)}_i w_{d+1} +\\ldots + x^{(d+k)}_i w_{d+k}+\\eps_i,$$\n",
    "\n",
    "где $x^{(d+1)}, \\ldots, x^{(d+k)}$ — новые признаки, добавленные Катей. Она также ищет вектор весов с помощью метода наименьших квадратов.\n",
    "\n",
    "После нахождения вектора весов каждая девушка вычислила RSS для своей модели (по обучающей выборке). \n",
    "1. Докажите, что RSS Кати оказался не больше RSS Маши.\n",
    "2. Могут ли RSS оказаться одинаковыми, но при этом ненулевыми? Если нет, докажите. Если да, приведите пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(впишите решение сюда)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5 (10 баллов)\n",
    "У рассеянного Александра из задачи 7 есть вектор ответов $(y_1, \\ldots, y_n)$ для задачи классификации, каждый $y_i\\in \\{0, 1\\}$, $n$ нечётное число, всего среди $y_i$ есть $m$ единиц и $(n-m)$ нулей. Как и в прошлый раз, Александр потерял матрицу признаков, поэтому вынужден использовать алгоритм, обучающийся только по ответам. Он хочет, чтобы алгоритм предсказывал вероятность $p$ получения единицы, и думает, какую функцию потерь ему выбрать из двух возможных:\n",
    "\n",
    "1. Log-loss: $$L_{LL}(y, p)=\\begin{cases}\n",
    "-\\log p,&\\text{ if }y=1;\\\\\n",
    "-\\log(1-p),&\\text{ if }y=0.\n",
    "\\end{cases}$$\n",
    "\n",
    "2. Абсолютное отклонение: $$L_{AD}(y, p)=|y-p|.$$\n",
    "\n",
    "Для нахождения оптимального $p$ Александр решает задачу минимизиации эмпирического риска:\n",
    "\n",
    "$$\\sum_{i=1}^n L(y_i, p) \\to \\min_{p},$$\n",
    "где $L$ — это либо $L_{LL}$, либо $L_{AD}$.\n",
    "\n",
    "Какое $p$ получится у Александра для каждой из данных функций потерь? Какую из функций потерь следует использовать, если Александр хочет, чтобы $p$ была состоятельной оценкой для вероятности получения единицы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(впишите решение сюда)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 6 (5 баллов)\n",
    "Кларисса решает задачу двухлассовой классификации (классы обозначаются $+1$ и $-1$) с помощью алгоритма машинного обучения, который для $i$-го объекта выдаёт степень уверенности $s_i$ алгоритма в том, что этот объект принадлежит к классу $+1$ (например, это может быть оценка вероятности, данная логистической регрессией). Кларисса выбирает пороговое значение $t$, после чего все объекты, для которых $s_i>t$, относит к положительному классу, а остальные — к отрицательному. Иными словами, окончательное предсказание классификатора имеет вид:\n",
    "$$\\hat y_i = [s_i>t] - [s_i\\le t].$$\n",
    "\n",
    "В таблице для каждого элемента обучающей выборки даны значения $s_i$ и их истинные классы. Построить ROC-кривую для данного алгоритма. (Вы можете сделать это вручную — это хорошее упражнение (посмотрите пример [здесь](https://github.com/esokolov/ml-course-hse/blob/master/2019-fall/seminars/sem05-linclass-metrics.pdf)) — либо самостоятельно написать код, который строит ROC-кривую. Использовать готовые решения из библиотек типа scikit-learn нельзя.)\n",
    "\n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "s_i & y_i \\\\\n",
    "\\hline\n",
    "0.6 & +1\\\\\n",
    "0.5 & -1\\\\\n",
    "0.1 & -1\\\\\n",
    "0.2 & -1\\\\\n",
    "0.4 & +1\\\\\n",
    "0.7 & +1\\\\\n",
    "\\hline\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(впишите решение сюда)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
